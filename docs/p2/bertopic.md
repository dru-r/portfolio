# Application of BERTopic to posts

## Summary
Topic modeling is one of the most common ways to analyse text. The methods I started with (Latent Dirichlet Allocation (LDA), non-negative matrix factorisation) operate on some form of bag-of-words or word count. While they still hold up well over many datasets, I was interested to try topic models that operated on embeddings (like Top2Vec, BERTopic). A big draw is their better ability to handle multilingual documents (though I do not do so here). I also hoped to find if they would work better on short text. <br><br>

I ran BERTopic with MPNet-v2 embeddings of the post text from r/FFXIV, r/ShitpostXIV, and r/ffxivdiscussion.

## Results
### 1. Topics returned
BERTopic doesn't require a topic count and automatically returns the number of topics as determined after. UMAP and HDBSCAN clustering. In this dataset, I was returned 661 topics, as shown below.<br><br>
![image](/imgs/topic_df.jpg)<br><br>
One of the issues I found was that a lot of posts got assigned to Topic -1 (which is the junk topic). BERTopic does allow for the calculation of probabilities for every post, which somewhat circumvents this issue. This is so as it returns probabilities for all topics for each post, making it possible to assign the highest probability topic in place of topic -1 to such posts. However, given the stochastic nature of UMAP and the sensitivity of HDBSCAN to its parameters, BERTopic also probably should be tuned further in such scenarios. While the results returned were pretty good with assigning the calculated highest probability, it made model training take much longer.<br><br>
The other thing I noticed is that 661 topics does seem like a fairly large number for just 54562 posts. There are many small topics at the minimum size of 10 posts (this is another tunable parameter). At first glance, they do seem to make sense still at least (e.g., topic 659 is about samurai’s kaiten skill). Generally, the topics overall all do seem to make some sense (e.g., of what appears to be a topic about black mages below). <br><br>
![image](/imgs/topic_eg.PNG)<br><br>

BERTopic also returns the very familiar intertopic distance map as generated by LDAvis for LDA.<br><br>
![image](/imgs/intertopic.PNG)<br>
[Interactive version here](/imgs/intertopic.html) <br><br>

We can also view inter-topic similarity (here, limited to the top 50 topics). <br><br>
![image](/imgs/topicsim.PNG)<br>
[Interactive version here](/imgs/topicsim.html) <br><br>
We see that most topics have very low similarity to each other. The only topic that appears to have a relatively larger similarity to most of the others is the quite general Topic 0, about questing and story/the main scenario quest. <br><br>

Finally, we can also understand how the topics might be hierarchically related. Having a hierarchy also means that we can use different ‘cuts’ of the hierarchy to decide on a lower n of topics, should we want to reduce the number of topics returned from BERTopic further. Only the top 100 topics shown below. <br><br>
![image](/imgs/hierarch.PNG)<br>
[Interactive version here](/imgs/hierarch_top100.html) <br><br>
We can see the utility of this; for e.g., topics 59, 31, and 3 (all some variation on spoilers) could be in the same clusters. Topics 51, 77, 21, 39, 19, and 68 (about leveling, experience via dailies, getting gil, the marketboard, gathering, and gearing – very ‘beginner player’ type of topics) can also potentially reduced to be in the same cluster.

### 2. Words in topics
BERTopic also returns a term score decline chart. As with LDA, topics in BERTopic are a list of representative words. The term score decline chart shows, per topic, the optimal number of words required for its representation (from the elbow method). <br><br>
![image](/imgs/tsdecline.PNG)<br>
[Interactive version here](/imgs/tsdecline.html) <br><br>
We see that for the majority of topics, it appears that the elbow is at 2 words. <br><br>

With this in mind, we can look at the topic word representations (only top 50 topics shown).<br>
![image](/imgs/topicbars.PNG)<br>
[Interactive version here](/imgs/topicbars.html) <br><br>
We can see distinct topics; e.g., about questing, job classes, housing, spoilers, friends/Discord, adventurer plates, etc. At the same time, some topics are less clear (e.g., Topic 5 – which is a combination of a character Zenos with the male pronoun). <br><br>

### 3. Topic frequency per subreddit
BERTopic also allows for topic usage comparison between classes (in this case, subreddits). This was the feature I was most interested in, given its (surface?) similarity to structural topic modeling (STM) ([used here]( https://dru-r.github.io/portfolio/p1/mature-vs-explicit.html). In BERTopic, it shows (1) the different vocab different classes of users might use for the same topic, and (2) frequency of the same topic across the different classes of users. This is quite parallel to STM’s ability to take metadata into account when calculating topic prevalence and topic content between classes. Only top 50 topics shown below. <br><br>
![image](/imgs/topicperclass.PNG)<br>
[Interactive version here](/imgs/topicperclass.html) <br><br>
Certain topics show a fair bit of difference between the different subreddits. For example, topic 4 (4_discord_fc_friends_looking). We see that the topic for r/ffxivdiscussion involves statics whereas r/ffxiv involves friends. <br><br>
In terms of prevalence, we see some topics do not appear in some subreddits at all, e.g., topic 10 (10_lalafell_lala_lalafells_lalas) only appears in r/shitpostxiv and r/ffxiv. This is likely due to the gameplay focus/’serious’-orientation of r/ffxivdiscussion. <br><br>
Finally, looking at overall frequency across the top 50 topics in the three subreddits, we can see each subreddit has its own topical focus. The main subreddit, r/ffxiv, focuses on questing, classes, housing, and spoiler-related material. r/ffxivdiscussion focuses largely on class discussion. r/shitpostxiv is more diverse in terms of its top topics, perhaps given its meme-sharing focus. 

### 4. Topic discussion over time
Lastly, BERTopic also allows us to see the change in topic terms and topic usage over time; similar to dynamic topic modeling. Again, only the top 50 topics are visualised. We can start by examining the non-normalised version. <br><br>
![image](/imgs/topicsovertime_notnorm.PNG)<br>
[Interactive version here](/imgs/ topicsovertime_notnorm.html) <br><br>
Recall that the dataset spans just from before Endwalker’s (patch 6.0) release to sometime in mid-May 2022 (patch 6.11). We see several peaks in the non-normalised chart that fits this sequence of events: <br>
1. Topic 8 (8_code_access_registration_endwalker) about the registration code for Endwalker was the most frequent topic at release, and dips quickly.<br>
2. The next peak is Topic 28 (28_grapes_grape_corvos_wine), which is likely about the low poly count of the grapes in a certain quest zone – this was picked up by players and subsequently memed on for a while. <br>
3. Then we see Topic 31 (31_lttrigger_taf_st3_crf) in early January 2022, which likely coincides with the release of patch 6.05. Topic 31 is less clear but might be related to the savage raids (based on words like p1s, tethers), which were released in this timeframe. <br>
4. We then see Topic 20 (20_server_oce_servers_data) in late January 2022, which is about the release of the Oceanic datacentre with patch 6.08 (January 25). <br>
5. The next major peaks happen in mid-ish April 2022, which coincide with patches 6.1 and 6.11. We see 3 key topics. First, Topic 2 (2_housing_lottery_house_plot), which is about the first-ever housing lottery and its difficulties. Second, Topic 6 (6_plate_plates_adventurer_adventure), which is about the then-new feature of allowing players to create namecards for their characters. And finally, Topic 18 (18_pvp_mode_cc_lb), which is about the new PvP mode Crystalline Conflict and the new PvP actions.<br> <br>

While the non-normalised chart gives us a good idea of these trends, it is difficult to see trends for topics which are just discussed much less frequently, as they are squished near y=0. The normalized chart gives us a better view of such topics. At the same time, fluctuations in the normalized chart are only meaningful within each topic and not between topics. With this normalization, we can see peaks of topics that were previously hidden (e.g., Topic 49, 49_transfer_home_world_congested, about the transferring of characters between servers – which coincides around the peak of the Oceanic datacentre release topic). <br><br>
![image](/imgs/topicsovertime_notnorm.PNG)<br>
[Interactive version here](/imgs/topicsovertime_notnorm.html)<br><br>

### Conclusion
This was a fun exercise in learning BERTopic through applying it to a social media dataset I’m more familiar with. There are things I’d like to follow up/improve on – getting less posts clustered under the junk topic Topic -1 and figuring out what type of cleaning is needed to get better results. Its creator has noted that [typically there is no need to preprocess the text](https://github.com/MaartenGr/BERTopic/issues/40) – however, they also acknowledge that cleaning might be needed for specific cases (e.g., HTML tags). That can definitely be done in this dataset.
